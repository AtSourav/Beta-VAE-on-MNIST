{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwaF6UgMloCikBeova/vHL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtSourav/CatsnDogs_VAE/blob/main/CatsnDogs_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to build a VAE in keras with the encoder and decoder based on CNNs."
      ],
      "metadata": {
        "id": "4ur1bU8y1uHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uoG7iO2skKtn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (160,160,3)\n",
        "batch_size = 256\n",
        "latent_dim = 64"
      ],
      "metadata": {
        "id": "Rc8U_BGj2L9P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = keras.Input(shape=input_size)\n",
        "\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", padding=\"valid\")(encoder_input)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)    # with strides=None this defaults to pool_size\n",
        "x = layers.BatchNormalization(axis=-1)(x) # the default data_format in the conv2d is \"channels last\", we want to normalize across the channels, hence we set axis=-1\n",
        "\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "\n",
        "x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "\n",
        "x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "\n",
        "x = layers.Flatten()(x)       # dimension of vector produced 8*8*256\n",
        "\n",
        "# we won't add any extra dense layers for now\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "def sampling(arg):\n",
        "  z_m, z_log_v = arg\n",
        "  batch = tf.shape(z_m)[0]\n",
        "  dim = tf.shape(z_m)[1]\n",
        "  eps = tf.random.normal(shape=(batch,dim))\n",
        "  return z_m + tf.exp(0.5*z_log_v)*eps\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean,z_log_var])   # we feed the sampling function in to a Lambda layer to build form a layer for the architecture as keras needs\n",
        "\n",
        "encoder = keras.Model(encoder_input, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaWmRig39WZE",
        "outputId": "9013814a-f265-489c-bc2b-7f0ce7a1dfd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 158, 158, 32  896         ['input_3[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 79, 79, 32)  0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 79, 79, 32)  128         ['max_pooling2d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 77, 77, 64)   18496       ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 64)  0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 38, 38, 64)  256         ['max_pooling2d_2[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 36, 36, 128)  73856       ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 128)  0          ['conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 18, 18, 128)  512        ['max_pooling2d_3[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 16, 16, 256)  295168      ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 256)   0           ['conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 8, 8, 256)   1024        ['max_pooling2d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 16384)        0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 64)           1048640     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 64)           1048640     ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 64)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,487,616\n",
            "Trainable params: 2,486,656\n",
            "Non-trainable params: 960\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of trainable parameters might just be too big for the dataset, possibility of overtraining, we'll see how it goes."
      ],
      "metadata": {
        "id": "d7reegoIckKu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60HAUzK8as95"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}