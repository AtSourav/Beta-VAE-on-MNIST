{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOA8UjFOweCi+HA4TlK3Zuv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AtSourav/CatsnDogs_VAE/blob/main/CatsnDogs_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to build a VAE in keras with the encoder and decoder based on CNNs."
      ],
      "metadata": {
        "id": "4ur1bU8y1uHM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uoG7iO2skKtn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (160,160,3)\n",
        "batch_size = 256\n",
        "latent_dim = 16"
      ],
      "metadata": {
        "id": "Rc8U_BGj2L9P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = keras.Input(shape=input_size)\n",
        "\n",
        "x = layers.Conv2D(16, 3, padding=\"valid\")(encoder_input)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)    # with strides=None this defaults to pool_size\n",
        "x = layers.BatchNormalization(axis=-1)(x) # the default data_format in the conv2d is \"channels last\", we want to normalize across the channels, hence we set axis=-1\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2D(32, 3, padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2D(64, 3, padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2D(128, 3, padding=\"valid\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=(2, 2), strides=None, padding=\"valid\")(x)\n",
        "x = layers.BatchNormalization(axis=-1)(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Flatten()(x)       # dimension of vector produced 8*8*256\n",
        "\n",
        "# we won't add any extra dense layers for now\n",
        "\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "def sampling(arg):\n",
        "  z_m, z_log_v = arg\n",
        "  batch = tf.shape(z_m)[0]\n",
        "  dim = tf.shape(z_m)[1]\n",
        "  eps = tf.random.normal(shape=(batch,dim))\n",
        "  return z_m + tf.exp(0.5*z_log_v)*eps\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean,z_log_var])   # we feed the sampling function in to a Lambda layer to build form a layer for the architecture as keras needs\n",
        "\n",
        "encoder = keras.Model(encoder_input, [z_mean, z_log_var, z], name='encoder')           # the second argument specifies that the encoder outputs [z_mean, z_log_var, z] for each input.\n",
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaWmRig39WZE",
        "outputId": "37d43a44-b035-4e4c-8f67-093428becc1d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 158, 158, 16  448         ['input_2[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 79, 79, 16)  0           ['conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 79, 79, 16)  64          ['max_pooling2d_1[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu (LeakyReLU)        (None, 79, 79, 16)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 77, 77, 32)   4640        ['leaky_re_lu[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 38, 38, 32)  0           ['conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 38, 38, 32)  128         ['max_pooling2d_2[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_1 (LeakyReLU)      (None, 38, 38, 32)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 36, 36, 64)   18496       ['leaky_re_lu_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_3 (MaxPooling2D)  (None, 18, 18, 64)  0           ['conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 18, 18, 64)  256         ['max_pooling2d_3[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_2 (LeakyReLU)      (None, 18, 18, 64)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 16, 16, 128)  73856       ['leaky_re_lu_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 128)   0           ['conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 8, 8, 128)   512         ['max_pooling2d_4[0][0]']        \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " leaky_re_lu_3 (LeakyReLU)      (None, 8, 8, 128)    0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 8192)         0           ['leaky_re_lu_3[0][0]']          \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 16)           131088      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 16)           131088      ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " lambda (Lambda)                (None, 16)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 360,576\n",
            "Trainable params: 360,096\n",
            "Non-trainable params: 480\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of trainable parameters might just be too big for the dataset, possibility of overtraining, we'll see how it goes. Let's make the decoder now."
      ],
      "metadata": {
        "id": "d7reegoIckKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_input = keras.Input(shape=(latent_dim,))\n",
        "\n",
        "x = layers.Dense(10*10*128)(latent_input)\n",
        "x = layers.LeakyReLU()(x)\n",
        "x = layers.Reshape((10,10,128))(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "                                                        # axis=-1 is set by default\n",
        "x = layers.Conv2DTranspose(32, 3, strides=2, padding='same')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(16, 3, strides=2, padding='same')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.LeakyReLU()(x)\n",
        "\n",
        "decoder_output = layers.Conv2DTranspose(3, 3, activation='sigmoid', strides=2, padding='same')(x)\n",
        "\n",
        "decoder = keras.Model(latent_input, decoder_output, name=\"decoder\")\n",
        "decoder.summary()"
      ],
      "metadata": {
        "id": "60HAUzK8as95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e4e19a0-37e1-46bd-8131-6edf53c8d749"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 16)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 12800)             217600    \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 12800)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 10, 10, 128)       0         \n",
            "                                                                 \n",
            " conv2d_transpose (Conv2DTra  (None, 20, 20, 64)       73792     \n",
            " nspose)                                                         \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 20, 20, 64)       256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 20, 20, 64)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_1 (Conv2DT  (None, 40, 40, 32)       18464     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 40, 40, 32)       128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 40, 40, 32)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_2 (Conv2DT  (None, 80, 80, 16)       4624      \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 80, 80, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 80, 80, 16)        0         \n",
            "                                                                 \n",
            " conv2d_transpose_3 (Conv2DT  (None, 160, 160, 3)      435       \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 315,363\n",
            "Trainable params: 315,139\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's instantiate the VAE by combininb the encoder and the decoder layers."
      ],
      "metadata": {
        "id": "KIIDWhu9NOTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_out = decoder(encoder(encoder_input)[2])          # we feed in z from the encoder output\n",
        "VAE = keras.Model(encoder_input, decoder_out, name='VAE')\n",
        "\n",
        "VAE.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jek1XQ2oNUnb",
        "outputId": "360e1eba-7768-4837-b71b-e7a6ed9d1859"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"VAE\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
            "                                                                 \n",
            " encoder (Functional)        [(None, 16),              360576    \n",
            "                              (None, 16),                        \n",
            "                              (None, 16)]                        \n",
            "                                                                 \n",
            " decoder (Functional)        (None, 160, 160, 3)       315363    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 675,939\n",
            "Trainable params: 675,235\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall define the loss function now, it's made of the reconstruction loss that tries to ensure repoducibility of the data, and the KL divergence loss that tries to ensure that the learned (approximate) posterior is close to the true posterior. Since we have continuous values for the pixels, we cannot use the Bernoulli log loss (binary cross entropy) for the reconstruction loss, we shall use mean squared error. In a different version, we shall try to implement a continuous version of the Bernoulli log loss based on 1907.06845.\n",
        "\n",
        "We shall keep a relative weight between the two terms in the total loss as a hyperparameter. We may want to investigate the effects of varying this hyperparameter eventually."
      ],
      "metadata": {
        "id": "uD87axE7vwtm"
      }
    }
  ]
}